# 词库训练简介 #

python-segment依赖于词库和词频工作，因此取得合适的词库就变成一个比较重要的问题。

# 准备工作 #

准备一些文本样本，一个足够大的初始词库。文本样本至少在1000W汉字以上，以utf-8格式存储，大约30M以上大小。推荐1亿汉字以上，以utf-8格式存储，大约100M。初始词库不应当小于30W词汇量，推荐50W以上。汉语常用词大约15W左右。

# 字频统计 #

使用cutter的frqstats功能，统计文本样本，得到字频表。通常字频表直接输出即可使用，一般中文常用字大约6000个上下。其余字字频计为1。 字频统计可以用于各种类别的txt上，得到不同的字频结果。不过通常来说意义不大。因为字频影响最大的是高频字，而汉语的高频字大多数情况下字频相对固定。

# 词库转换 #

初始词库通常是cedict格式或者是平文本格式，使用dbmgr的trans_plain或者trans_cedict功能，将其转换为初始带词频词表。

# 生成初始txt词库 #

将字频表和词库合并，就可以得到初始txt词库。

# 新词训练 #

首先导入初始txt词库，进行第一次新词训练。使用cutter的newtrains功能，可以得到一批高频未识别词和频率。注意，这一过程会耗费比较长的时间和内存。请准备一台好点的机器来做。人工阅览这些词，将其中不合适的删除，就可以得到带词频新词表。将这个新词表添加到初始txt词库后面，就可以得到未经词频训练的完整词库。 新词训练也适用于各种类别的文本素材，尤其是针对特性领域，使用新词训练理论上可以识别出一批新词，从而增强机器的分词准确性。然而注意，新词训练是基于不可分词语，对于在常见词基础上附加而成的新词识别能力很差，而且需要大量人力进行新词识别。

# 词频训练 #

使用cutter的frqtrains功能，对文本进行统计，可以得到词语在文本中的出现概率。注意，这一过程会耗费比较长的时间和内存。请准备一台好点的机器来做。训练完成后会自动保存词频。 词频训练对不同领域的文本素材有很强的针对性。在针对某个领域进行训练后，可以有效提高该领域的识别度。但是注意，如果词库不涵盖该领域所需词汇的话，词频训练效果很难体现。

# 词典输出和保存 #

marshal词典保存了相当多的数据结构，加载速度快。然而可读性差，文件大小大。因此可以用dbmgr的exportdb功能输出为txt格式词典，而后用gzip压缩。一般输出为txt词典会减小25%的大小，gzip压缩会减小66%的大小。以范例词典而言，压缩得到的词典为原始词典的1/4。

# 词典的合并 #

一般来说，用户的常用词典只有一个。但是基于某种理由，我们常常需要对词典进行合并，添加新词，或者补充训练结果。对词典的合并通常依照“词频调整-合并-重新训练词频”的步骤进行。

1. 首先调整词频，将两者词语平均出现概率调整到一致的地步。这步的最主要目地是为了防止后续训练时，某个词典中的词平均词频太小，导致被选中的概率相对偏小，从而出现聚集效应。
2. 词典合并，通过dbmgr的importdb指令进行词典合并。
3. 重训练，对合并后的词典，推荐将平均词频调整到1，重新进行词频训练。 

# 有效词的截取 #

一个词典内不是所有词都有同样的效用，某些词出现的概率会比其他词低很多，在整个识别过程中，多付出相当数量的资源而无法获得太多的效果提升。我们可以通过训练-词频截取的方法去除这些无效词。

1. 首先调整平均词频到1。
2. 针对某个样本进行词频训练，得到训练后词表。
3. 通过waterlevel指令进行水位查询，确定截取水位。通常推荐的截取水位为1.1，因为未出现的词语的默认次数为1。而在训练后词表中出现次数为1的词相当于在文本中未出现。
4. 通过shrink指令进行词频截取，截取结果会自动保存为工作词典。 

有效词的截取能够有效的增强系统的性能，减少词典内存占用，增加工作速度。但是去掉的无效词并不代表真的无效。去掉多余的无效词后，在识别的精度上会受到一定的影响。有效词的截取属于以精度换取资源的做法
